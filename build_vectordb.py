# build_vector_db.py
import json
import os
import torch
from typing import List
from langchain_core.documents import Document
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
import shutil

"""
<í•„ìš” ì„¤ì¹˜ ëª¨ë“ˆ>
pip install langchain-core
pip install langchain-community
pip install langchain-chroma
pip install langchain-text-splitters
pip install sentence-transformers
pip install chromadb
pip3 install torch torchvision torchaudio # (ê¶Œì¥) Apple Silicon/MPS ë˜ëŠ” CPUìš© PyTorch
"""

# â”€â”€ 0) ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ (MPS â†’ CUDA â†’ CPU)
if torch.backends.mps.is_available():
    DEVICE_STR = "mps"
elif torch.cuda.is_available():
    DEVICE_STR = "cuda"
else:
    DEVICE_STR = "cpu"
print(f"[Device] Using: {DEVICE_STR}")

def load_jsonl(path: str) -> List[dict]:
    """jsonl íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°"""
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                data.append(json.loads(line))
    return data


def auto_chunk_for_qa(docs: List[Document]) -> List[Document]:
    """
    Q+A ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©.
    ë¬¸ì„œ ê¸¸ì´ë¥¼ ë³´ê³  ìë™ìœ¼ë¡œ ì²­í‚¹ì„ ì ìš©:
      - ìµœëŒ€ ê¸¸ì´ <= 500ì â†’ ë¶„í•  ìŠ¤í‚µ
      - ê·¸ ì™¸ â†’ chunk_size=800, overlap=80 (â‰ˆ10%)
    """
    if not docs:
        return docs

    lengths = [len(d.page_content) for d in docs]
    max_len = max(lengths)

    if max_len <= 500:
        # í˜„ì¬ ë°ì´í„°ê°€ ì§§ì€ Q/A ìœ„ì£¼ë©´ ì²­í‚¹ ë¶ˆí•„ìš”
        print("âœ‚ï¸ Chunking: ìŠ¤í‚µ (max_len <= 500)")
        return docs

    print(f"âœ‚ï¸ Chunking: ì ìš© (max_len={max_len}) â†’ chunk_size=800, overlap=80")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=80,   #â‰ˆ10%
        separators=["\n\n", "\n", "ã€‚", ".", "!", "?", " ", ""]
    )
    chunked = splitter.split_documents(docs)
    print(f"   â†³ {len(docs)}ê°œ â†’ {len(chunked)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
    return chunked


def build_vector_db(jsonl_path: str, persist_dir: str = "vector_store", mode: str = "qa"):
    """
    mode = "qa"     â†’ Q + Aë¥¼ page_contentì— ì €ì¥ (ê¸¸ë©´ ìë™ ì²­í‚¹)
    mode = "q_only" â†’ Që§Œ page_content, AëŠ” metadataì— ì €ì¥ (ì²­í‚¹ ì—†ìŒ)
    """
    print(f"\nğŸš€ VectorDB êµ¬ì¶• ì‹œì‘ | mode={mode}")
    print(f"ğŸ“‚ {jsonl_path} ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    records = load_jsonl(jsonl_path)
    print(f"âœ… {len(records)}ê°œ ë ˆì½”ë“œ ë¡œë“œ ì™„ë£Œ")

    # 1) Document ë³€í™˜
    print("ğŸ“ Document ë³€í™˜ ì¤‘...")
    docs: List[Document] = []
    for i, item in enumerate(records, 1):
        q = str(item.get("Q", "")).strip()
        a = str(item.get("A", "")).strip()

        if mode == "q_only":
            # ê²€ìƒ‰ì€ ì§ˆë¬¸(Q) ê¸°ì¤€, ë‹µë³€(A)ëŠ” ë©”íƒ€ë°ì´í„°ë¡œ ë³´ê´€
            docs.append(Document(page_content=q, metadata={"type": "Q-A", "answer": a}))
        else:
            # ê¸°ë³¸: Q + A í•©ì³ì„œ ì €ì¥ â†’ ê¸¸ë©´ ì•„ë˜ì„œ ìë™ ì²­í‚¹
            docs.append(Document(page_content=f"Q: {q}\nA: {a}", metadata={"type": "Q-A", "question": q}))

        if i % 100 == 0:
            print(f"   â†³ {i}/{len(records)} ë³€í™˜ ì™„ë£Œ")
    print("âœ… Document ë³€í™˜ ì™„ë£Œ")

    # 2) (Q+A ëª¨ë“œì¼ ë•Œë§Œ) ìë™ ì²­í‚¹
    if mode == "qa":
        docs = auto_chunk_for_qa(docs)

    # 3) ì„ë² ë”© & ë²¡í„°DB ì €ì¥
    print("ğŸ” ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘... (nlpai-lab/KURE-v1)")
    embedding_model = SentenceTransformerEmbeddings(
        model_name="nlpai-lab/KURE-v1",
        model_kwargs={"device": DEVICE_STR},   # ì—¬ê¸°ì„œ ë””ë°”ì´ìŠ¤ ì§€ì •
        encode_kwargs={"batch_size": 64}       # ì ì ˆíˆ ì¡°ì • ê°€ëŠ¥
    )

    # ê¸°ì¡´ DB í´ë” ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
    if os.path.exists(persist_dir):
        #import shutil
        print(f"ğŸ—‘ ê¸°ì¡´ DB({persist_dir}) ì‚­ì œ ì¤‘...")
        shutil.rmtree(persist_dir)

    print(f"ğŸ’¾ Chroma ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥ ì¤‘... â†’ {persist_dir}")
    os.makedirs(persist_dir, exist_ok=True)
    vector_store = Chroma.from_documents(
        documents=docs,
        embedding=embedding_model,
        persist_directory=persist_dir
    )
    print(f"ğŸ‰ ì™„ë£Œ! ì´ {len(docs)}ê°œ(ì²­í¬ ê¸°ì¤€) ë¬¸ì„œë¥¼ {persist_dir}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. (mode={mode})")


# ì´ ë¸”ë¡ì€ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  ë•Œë§Œ ì‹¤í–‰ë¨
# ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ importí•  ë•ŒëŠ” ì‹¤í–‰ë˜ì§€ ì•Šì•„ DBê°€ ìë™ ì¬ìƒì„±ë˜ì§€ ì•ŠìŒ (ì´ëŸ´ ì¼ì€ ì‚¬ì‹¤ ì—†ìŒ)
if __name__ == "__main__":
    # Q+A ë°©ì‹ (ìë™ ì²­í‚¹)
    build_vector_db("data/combined_data.jsonl", persist_dir="vector_store_qa", mode="qa")

    # Q-only ë°©ì‹ (ì²­í‚¹ ì—†ìŒ)
    build_vector_db("data/combined_data.jsonl", persist_dir="vector_store_q_only", mode="q_only")